version: '3'

services:
  producer:
    build:
      context: .
      dockerfile: ./docker/Dockerfile
    command: ["./run.sh"]
    ports:
      - "5000:5000"
    environment:
      - "KAFKA_LOG_LEVEL=ERROR"
      - "LOG_LEVEL=INFO"
      - "DEVELOPMENT_MODE=True"
      - "NUM_THREADS=1"
      - "NUM_WORKERS=1"
      - "BOOTSTRAP_SERVERS_STRING=kafka1:9091"
    volumes:
      - ./src:/src
    networks:
      - producer-kafka
    depends_on:
      - kafka1

  nginx:
    image: nginx:1.15-alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx:/etc/nginx/conf.d
    networks:
      - producer-kafka
    depends_on:
      - producer

  zookeeper:
    container_name: zookeeper
    image: zookeeper:3.4.9
    ports:
      - "2181:2181"
    environment:
        ZOO_MY_ID: 1
        ZOO_PORT: 2181
        ZOO_SERVERS: server.1=zookeeper:2888:3888
    volumes:
      - zookeeper_data:/data
      - zookeeper_datalog:/datalog
    networks:
      - producer-kafka

  kafka1:
    container_name: kafka1
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9091:9091
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka1:19091,LISTENER_DOCKER_EXTERNAL://kafka1:9091
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
    volumes:
      - kafka1_volumes:/var/lib/kafka/data
    networks:
      - producer-kafka

  kafdrop:
    container_name: kafdrop
    image: obsidiandynamics/kafdrop:latest
    networks:
      - producer-kafka
    depends_on:
      - kafka1
    ports:
      - 19000:9000
    environment:
      KAFKA_BROKERCONNECT: kafka1:19091

  kafka_manager:
    container_name: kafka_manager
    image: sheepkiller/kafka-manager
    networks:
      - producer-kafka
    ports:
      - 9000:9000
    environment:
      - ZK_HOSTS=zookeeper:2181
    depends_on:
      - zookeeper

volumes:
  zookeeper_data:
  zookeeper_datalog:
  kafka1_volumes:

networks:
  producer-kafka:
    driver: "bridge"
    name: producer-kafka
  # spark:
  #   image: bitnami/spark:3
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   ports:
  #     - '8080:8080'
  #     - '7077:7077'
  #   networks:
  #     - producer-kafka

  # spark-worker-1:
  #   image: bitnami/spark:3
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   ports:
  #     - '8081:8081'
  #   networks:
  #     - producer-kafka

  # spark-worker-2:
  #   image: bitnami/spark:3
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   ports:
  #     - '8082:8081'
  #   networks:
  #     - producer-kafka

  # hadoop:
  #   image: cybermaggedon/hadoop
  #   networks:
  #     - producer-kafka
